{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "## the `iris` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `breast_cancer` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## the Horse Colic Dataset\n",
    "The data is from the [UCI database](https://archive.ics.uci.edu/ml/datasets/Horse+Colic). The data is loaded as follows. `?` represents missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data'\n",
    "df = pd.read_csv(url, delim_whitespace=True, header=None)\n",
    "df = df.replace(\"?\", np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will replace the missing by `0`. This part should be modified if you want to improve the performance of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0)\n",
    "X = df.iloc[:, 1:].to_numpy().astype(float)\n",
    "y = df[0].to_numpy().astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## the Dating dataset\n",
    "The data file can be downloaded from {Download}`here<./assests/datasets/datingTestSet2.txt>`. \n",
    "\n",
    "```{code-block} python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('datingTestSet2.txt', sep='\\t', header=None)\n",
    "X = np.array(df[[0, 1, 2]])\n",
    "y = np.array(df[3])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The dataset randomly generated\n",
    "- `make_moon` dataset\n",
    "\n",
    "This is used to generate two interleaving half circles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `make_gaussian_quantiles` dataset\n",
    "\n",
    "This is a generated isotropic Gaussian and label samples by quantile. \n",
    "\n",
    "The following code are from [this page](https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_twoclass.html#sphx-glr-auto-examples-ensemble-plot-adaboost-twoclass-py). It is used to generate a relative complex dataset by combining two datesets together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X1, y1 = make_gaussian_quantiles(cov=2.0, n_samples=200, n_features=2,\n",
    "                                 n_classes=2, random_state=1)\n",
    "X2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5, n_samples=300,\n",
    "                                 n_features=2, n_classes=2, random_state=1)\n",
    "X = np.concatenate((X1, X2))\n",
    "y = np.concatenate((y1, -y2 + 1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## `MNIST` dataset\n",
    "\n",
    "There are several versions of the dataset. \n",
    "\n",
    "- `tensorflow` provides the data with the original split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- asfasdfasfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `titanic` dataset\n",
    "This is the famuous Kaggle101 dataset. The original data can be download from [the Kaggle page](https://www.kaggle.com/competitions/titanic/data). You may also download the {Download}`training data<./assests/datasets/titanic/train.csv>` and the {Download}`test data<./assests/datasets/titanic/test.csv>` by click the link.\n",
    "\n",
    "```{code-block} python\n",
    "import pandas as pd\n",
    "dftrain = pd.read_csv('train.csv')\n",
    "dftest = pd.read_csv('test.csv')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dftrain = pd.read_csv('assests/datasets/titanic/train.csv')\n",
    "dftest = pd.read_csv('assests/datasets/titanic/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original is a little bit messy with missing values and mix of numeric data and string data. The above code reads the data into a DataFrame. The following code does some basic of preprocess. This part should be modified if you want to improve the performance of your model.\n",
    "\n",
    "1. Only select columns: `Pclass`, `Sex`, `Age`, `SibSp`, `Parch`, `Fare`. That is to say, `Name`, `Cabin` and `Embarked` are dropped.\n",
    "2. Fill the missing values in column `Age` and `Fare` by `0`.\n",
    "3. Replace the column `Sex` by the following map: `{'male': 0, 'female': 1}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def getnp(df):\n",
    "    df['mapSex'] = df['Sex'].map(lambda x: {'male': 0, 'female': 1}[x])\n",
    "    dfx = df[['Pclass', 'mapSex', 'Age', 'SibSp', 'Parch', 'Fare']].copy()\n",
    "    dfx['Fare'].fillna(0, inplace=True)\n",
    "    dfx['Age'].fillna(0, inplace=True)\n",
    "    if 'Survived' in df.columns:\n",
    "        y = df['Survived'].to_numpy()\n",
    "    else:\n",
    "        y = None\n",
    "    X = dfx.to_numpy()\n",
    "    return (X, y)\n",
    "\n",
    "X_train, y_train = getnp(dftrain)\n",
    "X_test, _ = getnp(dftest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of submitting to Kaggle, after getting `y_pred`, we could use the following file to prepare for the submission file.\n",
    "\n",
    "```{code-block} python\n",
    "def getdf(df, y):\n",
    "    df['Survived'] = y\n",
    "    return df[['PassengerId', 'Survived']]\n",
    "\n",
    "getdf(dftest, y_pred).to_csv('result.csv')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plant survival data with salt and microbe treatments\n",
    "\n",
    "This dataset is supported by DART SEED grant. It is provided by Dr. Suresh Subedi from ATU. The dataset is about the outcomes of certain treatments applied to plants. We would like to predict whether the plants survive based on the status of the plants and the treatments. The datafile can be downloaded from {Download}`here<assests/datasets/plants.xlsx>`.\n",
    "\n",
    "We could use the following code to read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('assests/datasets/plants.xlsx', engine='openpyxl', sheet_name='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few missing values. The missing values in `Outcome_after 12 months` are all `dead`. These are not recorded as `dead` because the cause of the death is more complicated and needs to be studied separatedly. In our case we could simply fill it with `dead`.\n",
    "\n",
    "There are two more missing values in `Stem diameter`. For simplicity we drop them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Outcome_after 12 months'].fillna('dead', inplace=True)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "Then we would like to transform the data. Here are the rules. \n",
    "\n",
    "- `Endophyte`: `I+`->`1`, `I-`->`-1`\n",
    "- `Treatment`: `Salt`->`1`, `Fresh`->`0`\n",
    "- `Tree_Replicate`: `T1`->`1`, `T2`->`2`, `T3`->`3`\n",
    "- `Outcome_after 12 months`: `survived`->`1`, `dead`->0\n",
    "\n",
    "Column `SN` will be dropped. \n",
    "\n",
    "Finally we put these together to get the features `X` and the label `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "df['Endophyte '] = df['Endophyte '].map({'I+': 1, 'I-': -1})\n",
    "df['Treatment'] = df['Treatment'].map({'Fresh': 0, 'Salt': 1})\n",
    "df['Tree_Replicate'] = df['Tree_Replicate'].str[1].astype(int)\n",
    "df['Outcome_after 12 months'] = df['Outcome_after 12 months'].map({'survived': 1, 'dead': 0})\n",
    "\n",
    "X = df.iloc[:, 1: -1].to_numpy()\n",
    "y = df['Outcome_after 12 months'].to_numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml22')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4eae2d79809986d0872e4e364459f0c9575ffff27a18380d5ee1c7bc910cc873"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
