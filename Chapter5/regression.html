
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>5.1. Basic idea &#8212; Machine Learning Notes - 2022 Fall</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="5.2. Regularization" href="regularization.html" />
    <link rel="prev" title="5. Logistic regression" href="intro.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Machine Learning Notes - 2022 Fall</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../references.html">
                    References
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Chapter1/intro.html">
   1. Introduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter1/what.html">
     1.1. What is Machine Learning?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter1/data.html">
     1.2. Basic setting for Machine learning problems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter1/pynotebook.html">
     1.3. Python quick guide
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter1/project.html">
     1.4. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Chapter2/intro.html">
   2. k-NN
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter2/knn.html">
     2.1. k-Nearest Neighbors Algorithm (k-NN)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter2/knnproj1.html">
     2.2. k-NN Project 1:
     <code class="docutils literal notranslate">
      <span class="pre">
       iris
      </span>
     </code>
     Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter2/knnproj2.html">
     2.3. k-NN Project 2: Dating Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter2/knnproj3.html">
     2.4. k-NN Project 3:
     <code class="docutils literal notranslate">
      <span class="pre">
       MNIST
      </span>
     </code>
     Handwritten recognition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter2/project.html">
     2.5. Exercises and Projects
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Chapter3/intro.html">
   3. Decision Tree
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter3/gini.html">
     3.1. Gini impurity
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter3/code.html">
     3.2. CART Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter3/dt.html">
     3.3. Decision Tree Project 1: the
     <code class="docutils literal notranslate">
      <span class="pre">
       iris
      </span>
     </code>
     dataset
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter3/dt2.html">
     3.4. Decision Tree Project 2:
     <code class="docutils literal notranslate">
      <span class="pre">
       make_moons
      </span>
     </code>
     dataset
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter3/project.html">
     3.5. Exercises and Projects
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Chapter4/intro.html">
   4. Ensemble methods
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter4/morevoting.html">
     4.1. Voting machine
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter4/randomforest.html">
     4.2. Bootstrap aggregating
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter4/adaboost.html">
     4.3.
     <code class="docutils literal notranslate">
      <span class="pre">
       AdaBoost
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Chapter4/project.html">
     4.4. Exercises and Projects
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="intro.html">
   5. Logistic Regression
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     5.1. Basic idea
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regularization.html">
     5.2. Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="nn.html">
     5.3. Neural network implement of Logistic regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="project.html">
     5.4. Exercises and Projects
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ChapterApp/intro.html">
   6. Appendix
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ChapterApp/datasets.html">
     6.1. Datasets
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/xiaoxl/ml22/master?urlpath=tree/docs/Chapter5/regression.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/Chapter5/regression.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sigmoid-function">
   5.1.1. Sigmoid function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-descent">
   5.1.2. Gradient descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-formulas">
   5.1.3. The Formulas
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#codes">
   5.1.4. Codes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#several-important-side-topics">
   5.1.5. Several important side topics
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#epochs">
     5.1.5.1. Epochs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#batch-gradient-descent-vs-sgd-vs-minibatch">
     5.1.5.2. Batch Gradient Descent vs SGD vs Minibatch
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Basic idea</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sigmoid-function">
   5.1.1. Sigmoid function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-descent">
   5.1.2. Gradient descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-formulas">
   5.1.3. The Formulas
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#codes">
   5.1.4. Codes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#several-important-side-topics">
   5.1.5. Several important side topics
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#epochs">
     5.1.5.1. Epochs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#batch-gradient-descent-vs-sgd-vs-minibatch">
     5.1.5.2. Batch Gradient Descent vs SGD vs Minibatch
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="basic-idea">
<h1><span class="section-number">5.1. </span>Basic idea<a class="headerlink" href="#basic-idea" title="Permalink to this headline">#</a></h1>
<p>The Logsitic regression is used to predict the probability of a data point belonging to a specific class. It is based on linear regression. The major difference is that logistic regreesion will have an activation function <span class="math notranslate nohighlight">\(\sigma\)</span> at the final stage to change the predicted values of the linear regression to the values that indicate classes. In the case of binary classification, the outcome of <span class="math notranslate nohighlight">\(\sigma\)</span> will be between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>, which is related to the two classes respectively. In this case, the number is interepted as the probability of the data to be in one of the specific class.</p>
<p>The model for Logistic regression is as follows:</p>
<div class="math notranslate nohighlight">
\[
p=\sigma(L(x))=\sigma\left(\theta_0+\sum_{j=1}^n\theta_jx_j\right)=\sigma\left(\Theta \hat{x}^T\right).
\]</div>
<p>In most cases, this activation function is chosen to be the Sigmoid funciton.</p>
<section id="sigmoid-function">
<h2><span class="section-number">5.1.1. </span>Sigmoid function<a class="headerlink" href="#sigmoid-function" title="Permalink to this headline">#</a></h2>
<p>The <em>Sigmoid</em> function is defined as follows:</p>
<div class="math notranslate nohighlight">
\[
\sigma(z)=\frac{1}{1+\mathrm{e}^{-z}}.
\]</div>
<p>The graph of the function is shown below.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../_images/regression_1_0.png" src="../_images/regression_1_0.png" />
</div>
</div>
<p>The main properties of <span class="math notranslate nohighlight">\(\sigma\)</span> are listed below as a Lemma.</p>
<div class="proof lemma admonition" id="sigmoid_property">
<p class="admonition-title"><span class="caption-number">Lemma 5.1 </span></p>
<section class="lemma-content" id="proof-content">
<p>The Sigmoid function <span class="math notranslate nohighlight">\(\sigma(z)\)</span> satisfies the following properties.</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\sigma(z)\rightarrow \infty\)</span> when <span class="math notranslate nohighlight">\(z\mapsto \infty\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma(z)\rightarrow -\infty\)</span> when <span class="math notranslate nohighlight">\(z\mapsto -\infty\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma(0)=0.5\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma(z)\)</span> is always increasing.</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma'(z)=\sigma(z)(1-\sigma(z))\)</span>.</p></li>
</ol>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. We will only look at the last one.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\sigma'(z)&amp;=-\frac{(1+\mathrm e^{-z})'}{(1+\mathrm e^{-z})^2}=\frac{\mathrm e^{-z}}{(1+\mathrm e^{-z})^2}=\frac{1}{1+\mathrm e^{-z}}\frac{\mathrm e^{-z}}{1+\mathrm e^{-z}}\\
&amp;=\sigma(z)\left(\frac{1+\mathrm e^{-z}}{1+\mathrm e^{-z}}-\frac{1}{1+\mathrm e^{-z}}\right)=\sigma(z)(1-\sigma(z)).
\end{split}
\end{split}\]</div>
</div>
</section>
<section id="gradient-descent">
<h2><span class="section-number">5.1.2. </span>Gradient descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">#</a></h2>
<p>Assume that we would like to minimize a function <span class="math notranslate nohighlight">\(J(\Theta)\)</span>, where this <span class="math notranslate nohighlight">\(\Theta\)</span> is an <span class="math notranslate nohighlight">\(N\)</span>-dim vector. Geometricly, we could treat <span class="math notranslate nohighlight">\(J\)</span> as a height function, and it tells us the height of the mountain. Then to minimize <span class="math notranslate nohighlight">\(J\)</span> is the same thing as to find the lowest point. One idea is to move towards the lowest point step by step. During each step we only need to lower our current height. After several steps we will be around the lowest point.</p>
<p>The geometric meaning of <span class="math notranslate nohighlight">\(\nabla J\)</span> is the direction that <span class="math notranslate nohighlight">\(J\)</span> increase the most. Therefore the opposite direction is the one we want to move in. The formula to update <span class="math notranslate nohighlight">\(x\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\Theta_{\text{new}} = \Theta_{\text{old}}-\alpha \nabla J(\Theta_{\text{old}}),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> is called the <em>learning rate</em> which controls how fast you want to learn. Usually if <span class="math notranslate nohighlight">\(\alpha\)</span> is small, the learning tends to be slow and stble, and when <span class="math notranslate nohighlight">\(\alpha\)</span> is big, the learning tends to be fast and unstable.</p>
<p>In machine learning, in most cases we would like to formulate the problem in terms of finding the lowest point of a <em>cost function</em> <span class="math notranslate nohighlight">\(J(\Theta)\)</span>. Then we could start to use Logistic regression to solve it. For binary classification problem, the cost function is defined to be</p>
<div class="math notranslate nohighlight">
\[
J(\Theta)=-\frac1m\sum_{i=1}^m\left[y^{(i)}\log(p^{(i)})+(1-y^{(i)})\log(1-p^{(i)})\right].
\]</div>
<p>Here <span class="math notranslate nohighlight">\(m\)</span> is the number of data points, <span class="math notranslate nohighlight">\(y^{(i)}\)</span> is the labelled result (which is either <span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span>), <span class="math notranslate nohighlight">\(p^{(i)}\)</span> is the predicted value (which is between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The algorithm gets its name since we are using the gradient to find a direction to lower our height.</p>
</div>
</section>
<section id="the-formulas">
<h2><span class="section-number">5.1.3. </span>The Formulas<a class="headerlink" href="#the-formulas" title="Permalink to this headline">#</a></h2>
<div class="proof theorem admonition" id="theorem-1">
<p class="admonition-title"><span class="caption-number">Theorem 5.1 </span></p>
<section class="theorem-content" id="proof-content">
<p>The gradient of <span class="math notranslate nohighlight">\(J\)</span> is computed by</p>
<div class="math notranslate nohighlight" id="equation-eqn-nablaj">
<span class="eqno">(5.1)<a class="headerlink" href="#equation-eqn-nablaj" title="Permalink to this equation">#</a></span>\[\nabla J =\frac1m(\textbf{p}-\textbf{y})^T\hat{\textbf{X}}.\]</div>
</section>
</div><div class="toggle docutils container">
<div class="proof admonition" id="proof">
<p>Proof. The formula is an application of the chain rule for the multivariable functions.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\dfrac{\partial p}{\partial \theta_k}&amp;=\dfrac{\partial}{\partial \theta_k}\sigma\left(\theta_0+\sum_{j=1}^n\theta_jx_j\right)=\dfrac{\partial}{\partial \theta_k}\sigma(L(\Theta))\\
&amp;=\sigma(L)(1-\sigma(L))\dfrac{\partial}{\partial \theta_k}\left(\theta_0+\sum_{j=1}^n\theta_jx_j\right)\\
&amp;=\begin{cases}
p(1-p)&amp;\text{ if }k=0,\\
p(1-p)x_k&amp;\text{ otherwise}.
\end{cases}
\end{split}
\end{split}\]</div>
<p>Then</p>
<div class="math notranslate nohighlight">
\[
\nabla p = \left(\frac{\partial p}{\partial\theta_0},\ldots,\frac{\partial p}{\partial\theta_n}\right) = p(1-p)\hat{x}.
\]</div>
<p>Then</p>
<div class="math notranslate nohighlight">
\[
\nabla \log(p) = \frac{\nabla p}p =\frac{p(1-p)\hat{x}}{p}=(1-p)\hat{x}.
\]</div>
<div class="math notranslate nohighlight">
\[
\nabla \log(1-p) = \frac{-\nabla p}{1-p} =-\frac{p(1-p)\hat{x}}{1-p}=-p\hat{x}.
\]</div>
<p>Then</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
\nabla J&amp; = -\frac1m\sum_{i=1}^m\left[y^{(i)}\nabla \log(p^{(i)})+(1-y^{(i)})\nabla \log(1-p^{(i)})\right]\\
&amp;=-\frac1m\sum_{i=1}^m\left[y^{(i)}(1-p^{(i)})\hat{x}^{(i)}+(1-y^{(i)})(-p^{(i)}\hat{x}^{(i)})\right]\\
&amp;=-\frac1m\sum_{i=1}^m\left[(y^{(i)}-p^{(i)})\hat{x}^{(i)}\right].
\end{split}
\end{split}\]</div>
<p>We write <span class="math notranslate nohighlight">\(\hat{x}^{(i)}\)</span> as row vectors, and stack all these row vectors vertically. What we get is a matrix <span class="math notranslate nohighlight">\(\hat{\textbf X}\)</span> of the size <span class="math notranslate nohighlight">\(m\times (1+n)\)</span>. We stack all <span class="math notranslate nohighlight">\(y^{(i)}\)</span> (resp. <span class="math notranslate nohighlight">\(p^{(i)}\)</span>) vectically to get the <span class="math notranslate nohighlight">\(m\)</span>-dim column vector <span class="math notranslate nohighlight">\(\textbf y\)</span> (resp. <span class="math notranslate nohighlight">\(\textbf p\)</span>).</p>
<p>Using this notation, the previous formula becomes</p>
<div class="math notranslate nohighlight">
\[
\nabla J =\frac1m(\textbf{p}-\textbf{y})^T\hat{\textbf{X}}.
\]</div>
<p>After the gradient can be computed, we can start to use the gradient descent method. Note that, although <span class="math notranslate nohighlight">\(\Theta\)</span> are not explicitly presented in the formula of <span class="math notranslate nohighlight">\(\nabla J\)</span>, this is used to modify <span class="math notranslate nohighlight">\(\Theta\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\Theta_{s+1} = \Theta_s - \alpha\nabla J.
\]</div>
</div>
</div>
</section>
<section id="codes">
<h2><span class="section-number">5.1.4. </span>Codes<a class="headerlink" href="#codes" title="Permalink to this headline">#</a></h2>
<p>We will only talk about using packages. <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> provides two methods to implement the Logistic regression. The API interface is very similar to other models.</p>
<p>Note that Logistic regression is very sensitive to the scale of features. Therefore we need to normalize the features before throwing them into the model.</p>
<p>Let’s still take <code class="docutils literal notranslate"><span class="pre">iris</span></code> as an example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.15</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The first method is <code class="docutils literal notranslate"><span class="pre">sklearn.linear_model.LogisticRegression</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>

<span class="n">steps</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;normalize&#39;</span><span class="p">,</span> <span class="n">MinMaxScaler</span><span class="p">()),</span>
         <span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">,</span> <span class="n">LogisticRegression</span><span class="p">())]</span>

<span class="n">log_reg</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">)</span>
<span class="n">log_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">log_reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.0
</pre></div>
</div>
</div>
</div>
<p>Note that this method has an option <code class="docutils literal notranslate"><span class="pre">solver</span></code> that will set the way to solve the Logistic regression problem, and there is no “stochastic gradient descent” provided. The default solver for this <code class="docutils literal notranslate"><span class="pre">LogsiticRegression</span></code> is <code class="docutils literal notranslate"><span class="pre">lbfgs</span></code> which will NOT be discussed in lectures.</p>
<p>The second method is <code class="docutils literal notranslate"><span class="pre">sklearn.linear_model.SGDClassifier</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>

<span class="n">steps</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;normalize&#39;</span><span class="p">,</span> <span class="n">MinMaxScaler</span><span class="p">()),</span>
         <span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">,</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;log_loss&#39;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">))]</span>

<span class="n">sgd_clf</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">)</span>
<span class="n">sgd_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">sgd_clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.0
</pre></div>
</div>
</div>
</div>
<p>This method is the one we discussed in lectures. The <code class="docutils literal notranslate"><span class="pre">log_loss</span></code> loss function is the binary entropy function we mentioned in lectures. If you change to other loss functions it will become other models.</p>
<p>From the above example, you may notice that <code class="docutils literal notranslate"><span class="pre">SGDClassifier</span></code> doesn’t perform as well as <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code>. This is due to the algorithm. To make <code class="docutils literal notranslate"><span class="pre">SGDClassifier</span></code> better you need to tune the hyperparameters, like <code class="docutils literal notranslate"><span class="pre">max_iter</span></code>, <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>/<code class="docutils literal notranslate"><span class="pre">alpha</span></code>, <code class="docutils literal notranslate"><span class="pre">penalty</span></code>, etc..</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The argument <code class="docutils literal notranslate"><span class="pre">warm_start</span></code> is used to set whether you want to use your previous model. When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, it will reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. The default is <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<p>Repeatedly calling <code class="docutils literal notranslate"><span class="pre">fit</span></code> when <code class="docutils literal notranslate"><span class="pre">warm_start</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code> can result in a different solution than when calling <code class="docutils literal notranslate"><span class="pre">fit</span></code> a single time because of the way the data is shuffled.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Note that for both methods, regularization (which will be discussed later) is applied by default.</p>
</div>
</section>
<section id="several-important-side-topics">
<h2><span class="section-number">5.1.5. </span>Several important side topics<a class="headerlink" href="#several-important-side-topics" title="Permalink to this headline">#</a></h2>
<section id="epochs">
<h3><span class="section-number">5.1.5.1. </span>Epochs<a class="headerlink" href="#epochs" title="Permalink to this headline">#</a></h3>
<p>We use epoch to describe feeding data into the model. One <em>Epoch</em> is when an entire dataset is passed through the model once. When using gradient descent, we tend to run several epochs. The number of maximal epochs is one important hyperparameter of this model.</p>
<p>The general idea is that more epochs are better for the score of the model, but it will definitely be slower. In addition, sometimes due to many other factors, after a few epochs, the model becomes stall. To train for more epochs cannot improve the model. In this case you have to turn to other methods.</p>
</section>
<section id="batch-gradient-descent-vs-sgd-vs-minibatch">
<h3><span class="section-number">5.1.5.2. </span>Batch Gradient Descent vs SGD vs Minibatch<a class="headerlink" href="#batch-gradient-descent-vs-sgd-vs-minibatch" title="Permalink to this headline">#</a></h3>
<p>Recall the Formula <a class="reference internal" href="#equation-eqn-nablaj">(5.1)</a>:</p>
<div class="math notranslate nohighlight">
\[
\nabla J =\frac1m(\textbf{p}-\textbf{y})^T\hat{\textbf{X}}.
\]</div>
<p>We could rewrite this formula:</p>
<div class="math notranslate nohighlight">
\[
\nabla J =\frac1m(\textbf{p}-\textbf{y})^T\hat{\textbf{X}}=\frac1m\sum_{i=1}^m\left[(p^{(i)}-y^{(i)})\hat{x}^{(i)}\right].
\]</div>
<p>This new formula can be understood in the following way: For every data point, we could get one gradient direction. Then <span class="math notranslate nohighlight">\(\nabla J\)</span> is the average of all gradient directions. So this algorithm can be expressed as that compute the gradient for every data points and then take the average, and finally update the parameters once. This algorithm is called <em>batch gradient descent</em>.</p>
<p>Following the idea, there is another way to update the model. For every data point, we could compute one gradient direction, and we could use the gradient direction to update the parameters of the model. This algorithm is called <em>stochastic gradient descent</em>.</p>
<p>Then there is an algrothm living in the middle, called <em>mini-batch gradient descent</em>. In this case, we will group the data set into a collection of subsets of a fiexed number of training examples. Each subset is called a <em>mini-batch</em>, and the fixed number of elements of each mini-batch is called the <em>batch size</em>. Using this method, we will just go through mini-batches one at a time, compute the average of the gradient for these data, and then update the parameters of the model after we finish one mini-batch. Assume that the total number of the dataset is <code class="docutils literal notranslate"><span class="pre">N</span></code>, the mini-batch size is <code class="docutils literal notranslate"><span class="pre">m</span></code>. Then there are <code class="docutils literal notranslate"><span class="pre">N/m</span></code> mini-batches, and during one epoch we will update the model <code class="docutils literal notranslate"><span class="pre">N/m</span></code> times.</p>
<p>Mini-batch size is one important hyperparameters of this model. Usually the larger the batch size is, the less variance the model has. Then it tends to behave more smoothly, but it will also be slower, and might be stuck to a local minimal. The smaller batch size is more chaotic. It might go faster, but it tends not to converge.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Chapter5"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="intro.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">5. </span>Logistic regression</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="regularization.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">5.2. </span>Regularization</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Xinli Xiao<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>